{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm, datasets\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import collections\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import RepeatedKFold \n",
    "from sklearn import metrics \n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "TCL=pd.read_excel('../../Documents/Full_List_Of_Features.xls')\n",
    "#Read in data\n",
    "\n",
    "#Do any data formatting here (i.e. remap strings as ints; drop unescessary columns)\n",
    "\n",
    "FeatureNames = list(TCL)\n",
    "FeatureNames=FeatureNames[2:]\n",
    "#Grab names for later\n",
    "\n",
    "#I like working in np\n",
    "TCLNP=TCL.to_numpy()\n",
    "\n",
    "#Outcomes\n",
    "y=TCLNP[:,1]\n",
    "#Features\n",
    "X=TCLNP[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso regression for feature selection with cross-validation \n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "#Lasso needs your variables to be normalized\n",
    "scaler = StandardScaler().fit(X_train) \n",
    "X_train_N = scaler.transform(X_train)\n",
    "X_test_N = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "lasso = LassoCV(cv=5, random_state=0, max_iter=10000)\n",
    "lasso.fit(X_train_N, y_train)\n",
    "print(lasso.alpha_)\n",
    "\n",
    "plt.semilogx(lasso.alphas_, lasso.mse_path_, \":\")\n",
    "plt.plot(\n",
    "    lasso.alphas_ ,\n",
    "    lasso.mse_path_.mean(axis=-1),\n",
    "    \"k\",\n",
    "    label=\"Average across the folds\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.axvline(\n",
    "    lasso.alpha_, linestyle=\"--\", color=\"k\", label=\"alpha: CV estimate\"\n",
    ")\n",
    "print(lasso.alpha_) #THIS IS THE VALUE YOU NEED\n",
    "plt.legend()\n",
    "plt.xlabel(\"alphas\")\n",
    "plt.ylabel(\"Mean square error\")\n",
    "plt.title(\"Mean square error on each fold\")\n",
    "plt.axis(\"tight\")\n",
    "#plt.savefig(\"ClinicalML-Predictions/Lasso_Regularlization_V1.pdf\", bbox_inches='tight', dpi=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "sel_ = SelectFromModel(\n",
    "    #C value below is inverse (^-1) of alpha value determined by 5x CV above)\n",
    "    LogisticRegression(C=0.822,penalty='l1', solver='liblinear'))\n",
    "sel_.fit(scaler.transform(X_train), y_train)\n",
    "arr=list(sel_.get_support())\n",
    "c=arr.count(True)\n",
    "print(c) #Prints total number of chosen features\n",
    "c=np.array(list(zip(arr, FeatureNames)))\n",
    "print(c) #This will print all features, with true next to the chosen ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the unchosen features\n",
    "for idx, decision in enumerate(c):\n",
    "    if decision[0]=='False':\n",
    "        TCL = TCL.drop(columns=[FeatureNames[idx]])\n",
    "#display(TCL)\n",
    "FeatureNames = list(TCL)\n",
    "FeatureNames=FeatureNames[2:]\n",
    "#regrab feature names\n",
    "\n",
    "#back to np\n",
    "TCLNP=TCL.to_numpy()\n",
    "\n",
    "\n",
    "y=TCLNP[:,1]\n",
    "X=TCLNP[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN THIS CELL \n",
    "#Manual Drop cell for any features removed by the SHAP dendrogram plot \n",
    "#TCL = TCL.drop(columns=['LAVI-Pre'])\n",
    "#TCL = TCL.drop(columns=['SA_PostNoVein'])\n",
    "#TCL = TCL.drop(columns=['Pre-A Fib Atrial Floor'])\n",
    "#TCLNP=TCL.to_numpy()\n",
    "\n",
    "#y=TCLNP[:,1]\n",
    "#X=TCLNP[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier\n",
    "#5-fold cross validation to ensure ~70/30 T/T split\n",
    "#Stratified to ensure equal distrubution of +/- samples \n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "#Random Forest\n",
    "classifier = RandomForestClassifier(bootstrap = True, max_features = 'auto')\n",
    "#Initializations\n",
    "tprs = []\n",
    "aucs = []\n",
    "precisions =[]\n",
    "recalls=[]\n",
    "thresholdss=[]\n",
    "y_real = []\n",
    "y_proba = []\n",
    "list_shap_values = list()\n",
    "list_test_sets = list()\n",
    "mean_precision = np.linspace(0,1,100)\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "i = 0\n",
    "for train, test in cv.split(X, y):\n",
    "    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    # Compute ROC AUC curve and PR Curve  \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y[test], probas_[:, 1], pos_label=1.0)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    precision, recall, thresholds = precision_recall_curve(y[test], probas_[:, 1], pos_label=1.0)\n",
    "    precisions.append(precision)\n",
    "    thresholdss.append(thresholds)\n",
    "    recalls.append(recall)\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #explainer=shap.KernelExplainer(model=classifier.predict_proba, data=X[train], link=\"logit\")\n",
    "    #Use tree explainer, I use model output=raw, but can be changed to logit. \n",
    "    explainer = shap.TreeExplainer(classifier, data=X[train].astype(float), feature_perturbation=\"interventional\", model_outut=\"raw\")\n",
    "    #I had check additivity to be false because I found that it was acting poorly b/c of a low threshold value and \n",
    "    #my small sample size. Should turn this back on to make sure everything is working properly.\n",
    "    shap_values=explainer.shap_values(X=X[test], check_additivity=False )\n",
    "    #for each iteration we save the test_set index and the shap_values\n",
    "    list_shap_values.append(shap_values)#####\n",
    "    list_test_sets.append(test)######\n",
    "    # Also save the real outcome and predicted outcome for sanity\n",
    "    y_real.append(y[test])\n",
    "    y_proba.append(probas_[:, 1])\n",
    "    #plotting stuff per CV\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "#plotting stuff overall\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = metrics.auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Random Forest',fontsize=18)\n",
    "plt.legend(loc=\"lower right\", prop={'size': 15})\n",
    "plt.show()\n",
    "#outputs ROC curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This plots PR curve. \n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import auc\n",
    "precision_array=[]\n",
    "recall_array = np.linspace(0, 1, 100)\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "reversed_mean_precision=0.0\n",
    "\n",
    "\n",
    "for i in range(0, 5):\n",
    "    precision_fold, recall_fold = precisions[i][::-1], recalls[i][::-1]  # reverse order of results\n",
    "    prec_array = np.interp(recall_array, recall_fold, precision_fold)\n",
    "    pr_auc = auc(recall_array, prec_array)\n",
    "    precision_array.append(prec_array)\n",
    "\n",
    "    reversed_mean_precision += np.interp(mean_recall, recall_fold, precision_fold)\n",
    "    reversed_mean_precision[0] = 0.0\n",
    "    \n",
    "    lab_fold = 'PR fold %d AUPR=%.4f' % (i+1, pr_auc)\n",
    "    plt.plot(recall_fold, precision_fold, alpha=0.3, label=lab_fold)\n",
    "    \n",
    "reversed_mean_precision /= 5\n",
    "reversed_mean_precision[0] = 1\n",
    "mean_auc_pr = auc(mean_recall, reversed_mean_precision)\n",
    "    \n",
    "\n",
    "mean_precision = np.mean(precision_array, axis=0)\n",
    "std_precision = np.std(precision_array, axis=0)\n",
    "plt.fill_between(recall_array, mean_precision + std_precision, mean_precision - std_precision, alpha=0.3, linewidth=0, color='grey')\n",
    "#plt.title(\"PR curves; {} folds\".format(k_fold.n_splits), weight=\"bold\", fontsize=15)\n",
    "plt.plot(mean_recall, ([reversed_mean_precision])[0],\n",
    "         label='Mean PR (AUC = %0.2f  $\\pm$ %0.2f)' % (mean_auc_pr, mean(std_precision)), lw=2, color='blue')  \n",
    "plt.xlabel(\"Recall (Sensitivity)\", fontsize=18)\n",
    "plt.ylabel(\"Precision (PPV)\", fontsize=18)\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "\n",
    "plt.legend(loc=\"lower right\", prop={'size': 18})\n",
    "plt.title(\"PR Curve\", fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will plot your dendogram to assess feature redundancy via shap. See https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%C2%A0insights.html\n",
    "#Cutoff can be changed via clustering_cutoff\n",
    "#Sometimes the implementation of shap_values is inconsistent, (this is somwhat documented). I manually mutate the \n",
    "#shap values here to get it into a useable format. \n",
    "explainer = shap.Explainer(classifier)\n",
    "shap_values2 = explainer(X)\n",
    "shap_values2.values=shap_values[0]\n",
    "clust = shap.utils.hclust(X, y, linkage=\"single\")\n",
    "shap.plots.bar(shap_values2, clustering_cutoff=1, clustering=clust,  show=True)\n",
    "#plt.savefig(\"V1F/Barplot.pdf\", bbox_inches='tight', dpi=800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile SHAP values \n",
    "#Here I format the variable shap_values as well. If shap_values[1] in the instances below doesnt work, you can try \n",
    "#the variable shap_values2 described above. \n",
    "shap.initjs()\n",
    "test_set = list_test_sets[0]\n",
    "shap_values = np.array(list_shap_values[0])\n",
    "for i in range(1,len(list_test_sets)):\n",
    "    test_set = np.concatenate((test_set,list_test_sets[i]),axis=0)\n",
    "    shap_values = np.concatenate((shap_values,np.array(list_shap_values[i])),axis=1)\n",
    "X_test = X[test_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "solved-parking",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3e356f730d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Various means to vizualize data -- run seprately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Beeswarm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFeatureNamesHoldout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_display\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#Dependence plot, where first value is index of feature you want to see, here is 5.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdependence_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshap_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFeatureNamesHoldout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteraction_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, xmin=50000, xmax=250000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shap' is not defined"
     ]
    }
   ],
   "source": [
    "#Various means to vizualize data -- run each in a seperate cell.  \n",
    "#Beeswarm\n",
    "shap.summary_plot(shap_values[1], X_test, feature_names=FeatureNamesHoldout, max_display=30, show=False)\n",
    "#Dependence plot, where first value is index of feature you want to see, here is 5. \n",
    "shap.dependence_plot(5, shap_values[1], X_test, show=False, feature_names=FeatureNamesHoldout, interaction_index=9)#, xmin=50000, xmax=250000)\n",
    "#Force plot where \"6\" is the index of the case you want to see within your testing set. \n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][6,:], X_test[6,:],feature_names=FeatureNames, plot_cmap=['#FFD300','#A1045A'], matplotlib=False, link=\"identity\")\n",
    "#waterfall plot (not used in paper)\n",
    "shap.plots._waterfall.waterfall_legacy(explainer.expected_value[1], shap_values[0][1], X[test][1])\n",
    "#Bar plot of mean importance\n",
    "shap.summary_plot(shap_values[1], X_test, plot_type=\"bar\", feature_names=FeatureNamesHoldout, max_display=30, show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure to save everything\n",
    "import dill                            #pip install dill --user\n",
    "filename = 'globalsave_V1F-2.pkl'\n",
    "dill.dump_session(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and to load the session again:\n",
    "\n",
    "import dill\n",
    "filename = 'globalsave_V1F-2.pkl'\n",
    "\n",
    "dill.load_session(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
